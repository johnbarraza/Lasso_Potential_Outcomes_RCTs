{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0aca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats \n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aec894",
   "metadata": {},
   "source": [
    "## 3.1 Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b48bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seteo básico\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "X1 = np.random.normal(loc=15, scale=5, size=n)  # Continua\n",
    "X2 = np.random.normal(loc=10, scale=2, size=n)  # Continua\n",
    "X3 = np.random.binomial(n=1, p=0.6, size=n)     # Binaria\n",
    "X4 = np.random.binomial(n=1, p=0.25, size=n)    # Binaria\n",
    "\n",
    "# Generamos la asignación al tratamiento D ~ Bern(0.5)\n",
    "D = np.random.binomial(n=1, p=0.5, size=n)\n",
    "#print(D)\n",
    "\n",
    "epsilon = np.random.normal(loc=0, scale=1, size=n) \n",
    "Y = 2*D + 0.5*X1 - 0.3*X2 + 0.2*X3 + epsilon\n",
    "#print(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos dataframe\n",
    "data = {\n",
    "    'Y': Y,\n",
    "    'D': D,\n",
    "    'X1': X1,\n",
    "    'X2': X2,\n",
    "    'X3': X3,\n",
    "    'X4': X4\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos los datos en grupo de tratamiento y control para la comparación\n",
    "treatment_group = df[df['D'] == 1]\n",
    "control_group = df[df['D'] == 0]\n",
    "\n",
    "#print(treatment_group)\n",
    "#print(control_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc728c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = ['X1', 'X2', 'X3', 'X4']\n",
    "\n",
    "# Iteramos sobre cada covariable para realizar el t-test\n",
    "for cov in covariates:\n",
    "    # Comparamos la media de la covariable 'cov' en el grupo de tratamiento vs. control\n",
    "    t_stat, p_value = stats.ttest_ind(treatment_group[cov], control_group[cov])\n",
    "    \n",
    "    print(f\"Covariable: {cov}\")\n",
    "    print(f\"  Media Grupo Tratamiento: {treatment_group[cov].mean():.4f}\")\n",
    "    print(f\"  Media Grupo Control:     {control_group[cov].mean():.4f}\")\n",
    "    print(f\"  P-value:                 {p_value:.4f}\")\n",
    "    \n",
    "    if p_value > 0.05:\n",
    "        print(\"  No hay diferencia estadística. grupos  balanceados\")\n",
    "    else:\n",
    "        print(\"   Hay una diferencia estadística. grupos NO  balanceados\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c686d1b",
   "metadata": {},
   "source": [
    "## 3.2 Estimating the Average Treatment Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab1d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate the treatment effect (ATE) using a simple reg:\n",
    "y = df['Y']\n",
    "X_simple = df[['D']]\n",
    "#con constante:\n",
    "X_simple = sm.add_constant(X_simple) \n",
    "model_simple = sm.OLS(y, X_simple).fit()\n",
    "print(model_simple.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff9cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                      Y   R-squared (uncentered):                   0.976\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.976\n",
      "Method:                 Least Squares   F-statistic:                              8017.\n",
      "Date:                Mon, 22 Sep 2025   Prob (F-statistic):                        0.00\n",
      "Time:                        22:38:27   Log-Likelihood:                         -1401.2\n",
      "No. Observations:                1000   AIC:                                      2812.\n",
      "Df Residuals:                     995   BIC:                                      2837.\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "D              2.0432      0.062     32.979      0.000       1.922       2.165\n",
      "X1             0.4978      0.005     91.664      0.000       0.487       0.508\n",
      "X2            -0.3027      0.009    -33.500      0.000      -0.320      -0.285\n",
      "X3             0.2358      0.061      3.836      0.000       0.115       0.356\n",
      "X4             0.0664      0.075      0.891      0.373      -0.080       0.213\n",
      "==============================================================================\n",
      "Omnibus:                        1.107   Durbin-Watson:                   2.027\n",
      "Prob(Omnibus):                  0.575   Jarque-Bera (JB):                0.991\n",
      "Skew:                          -0.017   Prob(JB):                        0.609\n",
      "Kurtosis:                       3.150   Cond. No.                         44.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# estimo ATE controlando por todas las covariables\n",
    "X_multiple = df[['D', 'X1', 'X2', 'X3', 'X4']]\n",
    "#AGREGO LA CONSTANTE:\n",
    "X_multiple = sm.add_constant(X_multiple) \n",
    "model_multiple = sm.OLS(y, X_multiple).fit()\n",
    "print(model_multiple.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a8e76",
   "metadata": {},
   "source": [
    "Los valores del ATE no varian mucho, los valores que tomó fue de 2.2014 y  2.0606 para el modelo simple y para el modelo múltiple respectivamente. Mientras que el valor verdadero es 2. Esto se debe a que la asignación del D fue aletoria y no esta correlacionada con las X.\n",
    "\n",
    "Acerca de lo que pasa con los errores estandares es que bajaron de 0.173 para el modelo simple y 0.062 el modelo multiple. Lo que pasa es que las covariables son buenas predictoras del Y (asi es nuestro PGD), por lo que al incluirlos reduicmos la magnitud del error residual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487939b3",
   "metadata": {},
   "source": [
    "## 3.3 LASSO and Variable Selection (3 points)\n",
    "\n",
    "### 3.3.1 Use LASSO to select covariates that predict Y (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca259693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X1', 'X2', 'X3']\n"
     ]
    }
   ],
   "source": [
    "#definimos las covariables\n",
    "X_lasso = df.drop(columns=['Y', 'D'])\n",
    "y_lasso = df['Y']\n",
    "lasso_cv = LassoCV(cv=10, random_state=42).fit(X_lasso, y_lasso)\n",
    "\n",
    "coefficients = lasso_cv.coef_\n",
    "selected_mask = coefficients != 0  #variables eleggidas osn las que son dif de cero\n",
    "selected_covariates = X_lasso.columns[selected_mask].tolist()\n",
    "#print(selected_covariates)\n",
    "\n",
    "#variables selecionadas\n",
    "print(f\"El alpha (lambda_min) óptimo encontrado por CV es: {lasso_cv.alpha_:.4f}\")\n",
    "print(\"\\nCoeficientes de LASSO para cada covariable:\")\n",
    "for name, coef in zip(X_lasso.columns, coefficients):\n",
    "    print(f\"  {name}: {coef:.4f}\")\n",
    "\n",
    "print(f\"\\n✅ Covariables seleccionadas por LASSO: {selected_covariates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2177a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.889\n",
      "Model:                            OLS   Adj. R-squared:                  0.888\n",
      "Method:                 Least Squares   F-statistic:                     1990.\n",
      "Date:                Mon, 22 Sep 2025   Prob (F-statistic):               0.00\n",
      "Time:                        22:49:40   Log-Likelihood:                -1399.4\n",
      "No. Observations:                1000   AIC:                             2809.\n",
      "Df Residuals:                     995   BIC:                             2833.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.4175      0.199     -2.094      0.037      -0.809      -0.026\n",
      "D              2.0565      0.062     33.040      0.000       1.934       2.179\n",
      "X1             0.5050      0.006     79.121      0.000       0.492       0.518\n",
      "X2            -0.2745      0.016    -17.576      0.000      -0.305      -0.244\n",
      "X3             0.2706      0.063      4.262      0.000       0.146       0.395\n",
      "==============================================================================\n",
      "Omnibus:                        1.241   Durbin-Watson:                   2.018\n",
      "Prob(Omnibus):                  0.538   Jarque-Bera (JB):                1.136\n",
      "Skew:                          -0.018   Prob(JB):                        0.567\n",
      "Kurtosis:                       3.161   Cond. No.                         121.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#reestimando ATE con las varibles seleccionadas por LASSO\n",
    "X_post_lasso = df[['D'] + selected_covariates]\n",
    "y = df['Y'] \n",
    "\n",
    "# Agregamos la constante y ajustamos el modelo\n",
    "X_post_lasso = sm.add_constant(X_post_lasso)\n",
    "model_post_lasso = sm.OLS(y, X_post_lasso).fit()\n",
    "print(model_post_lasso.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e57d38",
   "metadata": {},
   "source": [
    "La precicisón de Lasso es casi identica a usar el modelo completo, el coeficiente de LASSO del D es 2.0565 y el de completo es 2.0432, mientras que el error estandar es el mismo para ambos 0.062. Esto se debe a que Lasso hizo un buen trabajo eliminando X4 que era variable irrelevante del modelo (recordemos que el PGD no depende de X4), es decir logra la casi la misma precisión que el modelo completo sin necesidad de agregar variables innecesarias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
